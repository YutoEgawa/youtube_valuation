import streamlit as st
import pandas as pd
import re
from datetime import datetime
import numpy as np
import math
import urllib.parse
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseUpload
import io
import json

creds_dict = json.loads(st.secrets["gcp_service_account"])
creds = service_account.Credentials.from_service_account_info(creds_dict)

# ğŸŒ Language switcher
language = st.radio("ğŸŒ Select Language / è¨€èªã‚’é¸ã‚“ã§ãã ã•ã„", ("Japanese", "English"))

labels = {
    "Japanese": {
        "title": "YouTube Channelè³‡ç”£ä¾¡å€¤æŸ»å®š",
        "channelname": "ãƒãƒ£ãƒ³ãƒãƒ«åï¼ˆä»»æ„ï¼‰",
        "upload": "æœˆåˆ¥ãƒ»å‹•ç”»åˆ¥ã®å†ç”Ÿå›æ•°ã‚’CSVå½¢å¼ã§ã”è‡ªèº«ã®Youtube Studioã‹ã‚‰å–å¾—ã—ä»¥ä¸‹ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚éå»ï¼–ãƒ¶æœˆåˆ†ã®ãƒ•ã‚¡ã‚¤ãƒ«ã€ã¤ã¾ã‚Šï¼–ã¤ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã„ãŸã ã‘ã‚Œã°æœ€ã‚‚æ­£ç¢ºãªæŸ»å®šçµæœã‚’ã”æä¾›ã„ãŸã—ã¾ã™ã€‚",
        "short_result": "shortå‹•ç”»åˆ†é¡å¾Œã®ãƒ‡ãƒ¼ã‚¿",
        "future_views": "å°†æ¥ã®å†ç”Ÿå›æ•°ï¼ˆäºˆæ¸¬ï¼‰ä»˜ããƒ‡ãƒ¼ã‚¿",
        "value_table": "DCFè³‡ç”£ä¾¡å€¤ï¼ˆå‹•ç”»å˜ä½ï¼‰",
        "channel_value": "ğŸ“ˆ ãƒãƒ£ãƒ³ãƒãƒ«å…¨ä½“ã®è³‡ç”£ä¾¡å€¤ï¼š{:,} å††",
        "share": "ğŸ•Š Xã§ã‚·ã‚§ã‚¢ã™ã‚‹ï¼ˆæ—§Twitterï¼‰",
        "download": "ğŸ’¾ å‹•ç”»ã”ã¨ã®è³‡ç”£ä¾¡å€¤ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        "tweet_text": "ç§ã®YouTubeãƒãƒ£ãƒ³ãƒãƒ«ã®è³‡ç”£ä¾¡å€¤ã¯ {:,} å††ã§ã—ãŸï¼ğŸ“ˆ\n\n#YouTube #è³‡ç”£ä¾¡å€¤ #å‹•ç”»åˆ†æ",
        "contact":"ğŸ“© ã”ä¸æ˜ç‚¹ãŒã‚ã‚Œã° [your-email@example.com](mailto:your-email@example.com) ã¾ã§ãŠæ°—è»½ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚",
        "privacy_policy": "ğŸ”å½“ã‚¢ãƒ—ãƒªã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã®ãŸã‚ã«ä¸€æ™‚çš„ã«ä½¿ç”¨ã—ã¾ã™ã€‚\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åŒæ„ãªã—ã«ã€æƒ…å ±ã‚’ç¬¬ä¸‰è€…ãŒé–²è¦§ã§ãã‚‹çŠ¶æ…‹ã«ç½®ãã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚" ,  
        "analyzing": "ğŸ“Š CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¦ã„ã¾ã™..." ,  
        "finished": "ã©ã†ã§ã™ã‹ï¼Ÿ" ,  
        "no_date_file": "âš ï¸ CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åå‰ã‹ã‚‰å¹´æœˆã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«åã¯YYYY_MMã«çµ±ä¸€ã—ã¦ãã ã•ã„ã€‚" , 
        "no_avail_file": "â—ï¸æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", 
    },

    "English": {
        "title": "YouTube Channel Asset Valuation App",
        "channelname": "Channel Name (optional)",
        "upload": "Upload your monthly view-counts by video downloadable from your YouTube Studio. To offer most accurate result, please upload the latest 6 months data, i.e. 6 csv files below.",
        "short_result": "Short Video Classification Result",
        "future_views": "Future Monthly Views (Projected)",
        "value_table": "DCF Asset Value per Video",
        "channel_value": "ğŸ“ˆ Total Channel Asset Value: Â¥{:,}",
        "share": "ğŸ•Š Share on X (Twitter)",
        "download": "ğŸ’¾ Download CSV of Asset Value per Video",
        "tweet_text": "My YouTube channel is worth Â¥{:,}! ğŸ“ˆ\n\n#YouTube #AssetValuation #CreatorEconomy",
        "contact":"ğŸ“© Contact us: [your-email@example.com](mailto:your-email@example.com)" ,
        "privacy_policy":" ğŸ”This app uses uploaded CSV files only for processing purposes. No data will be shared externally without consent." ,
        "analyzing": "ğŸ“Š analyzing CSV file..." ,  
        "finished": "how do you find this service?" ,  
        "no_date_file": "âš ï¸ failed to extract date from your CSV file name . Please format the file name as 'YYYY_MM'." , 
        "no_avail_file": "â—ï¸no files available", 
    }
}

text = labels[language]

st.title(text["title"])

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰éƒ¨åˆ†
uploaded_files = st.file_uploader(
    text["upload"],
    type="csv",
    accept_multiple_files=True
)

# æ—¥æœ¬èªâ†’è‹±èªã¸ã®ã‚«ãƒ©ãƒ åå¤‰æ›ãƒãƒƒãƒ—ï¼ˆå¿…è¦ãªåˆ†ã ã‘ï¼‰
rename_dict = {
    'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„': 'videoId',
    'å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«': 'title',
    'å‹•ç”»å…¬é–‹æ™‚åˆ»': 'publishedAt',
    'é•·ã•': 'duration',
    'è¦–è´å›æ•°': 'viewCount',
    'æ–°ã—ã„è¦–è´è€…æ•°': 'newViewers',
    'ç™»éŒ²è€…å¢—åŠ æ•°': 'subscriberGain',
}

# ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
def extract_year_month(filename):
    match = re.search(r"(\d{4})[^\d]?(\d{2})", filename)
    if match:
        return match.group(1), match.group(2)
    return None, None

# CSVã‚’å‡¦ç†ã™ã‚‹é–¢æ•°
def process_csv(file):
    df = pd.read_csv(file)
    
    # âœ… è‡ªå‹•åˆ¤å®šï¼šæ—¥æœ¬èªã®ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    is_japanese = any(col in rename_dict for col in df.columns)

    # ğŸ¯ æ—¥æœ¬èªã®å ´åˆã®ã¿ã‚«ãƒ©ãƒ åå¤‰æ›
    if is_japanese:
        df.rename(columns=rename_dict, inplace=True)

    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¹´æœˆå–å¾—
    year, month = extract_year_month(file.name)
    if not year or not month:
        st.warning(f"âš ï¸ {file.name} ã‹ã‚‰å¹´æœˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None

    # âœ… åˆè¨ˆè¡Œã¯é™¤å¤–
    df = df[df["videoId"] != "åˆè¨ˆ"]

    # âœ… å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘æ®‹ã™
    target_cols = ['videoId', 'title', 'publishedAt', 'duration', 'viewCount']
    # ğŸ¯ å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã ã‘æ®‹ã™ï¼ˆæ—¥æœ¬èªãƒ»è‹±èªã©ã¡ã‚‰ã§ã‚‚å‹•ä½œï¼‰
    df = df[[col for col in target_cols if col in df.columns]]

    # âœ… ã‚«ãƒ©ãƒ åã‚’å¹´æœˆä»˜ãã«å¤‰æ›´ï¼ˆè‹±èªã§ã‚‚ãã®ã¾ã¾é©ç”¨ï¼‰
    df.rename(columns={
        'viewCount': f'viewCount_{year}_{month}',
    }, inplace=True)

    return df

# Function to parse publishedAt with multiple formats
def parse_published_at(date_str):
    for fmt in ('%Y-%m-%dT%H:%M:%SZ', '%b %d, %Y'):  # Supports both formats
        try:
            return datetime.strptime(date_str, fmt)
        except ValueError:
            continue
    return None  # Return None if no format matches

# Function to classify videos as "short" or not
def is_short(row):
    if row['publishedAt'] is None or pd.isna(row['duration']):
        return 0  # Handle missing or invalid dates/durations

    try:
        duration_sec = float(row['duration'])  # Convert duration to a number (seconds)
    except ValueError:
        return 0  # If duration is not numeric, assume it's not a short

    # Define the classification rules
    cutoff_date = datetime(2024, 10, 15)

    if row['publishedAt'] < cutoff_date:
        return 1 if duration_sec <= 60 else 0
    else:
        return 1 if duration_sec <= 179 else 0  # 2 minutes 59 seconds = 179 seconds
def project_future_views(df_merged, growth_slowdown_1st, growth_slowdown_2nd, growth_slowdown_steady, growth_speculation):
    """
    Projects future monthly view counts based on historical data.

    Parameters:
        df_merged (pd.DataFrame): The DataFrame containing video view counts.
        growth_slowdown_1st (float): Growth slowdown one month after publish.
        growth_slowdown_2nd (float): Growth slowdown two months after publish.
        growth_slowdown_steady (float): Steady-state growth.
        growth_speculation (float): Additional speculative growth for acquisitions.

    Returns:
        pd.DataFrame: The DataFrame with projected view counts added.
    """

    # Identify view count columns
    monthly_view_count_columns = [col for col in df_merged.columns if col.startswith("viewCount_")]

    # Count valid (non-null) monthly view counts
    num_valid_values = df_merged[monthly_view_count_columns].notna().sum(axis=1)

    # Extract the latest date from column names
    latest_date_str = "_".join(monthly_view_count_columns[-1].split("_")[1:])
    latest_date = pd.to_datetime(latest_date_str, format="%Y_%m")  # Convert to datetime

    # Iterate through each row and project future view counts
    for index, valid_count in enumerate(num_valid_values):
        if valid_count == 1:
            base_col = monthly_view_count_columns[-1]
            base_value = df_merged.loc[index, base_col]

            projected_value = base_value * growth_slowdown_1st  # First projection
            for i, growth_factor in enumerate([growth_slowdown_1st, growth_slowdown_2nd, growth_slowdown_steady], start=1):
                future_date = latest_date + pd.DateOffset(months=i)
                future_col = f"viewCount_{future_date.strftime('%Y_%m')}E"
                df_merged.loc[index, future_col] = projected_value
                projected_value *= growth_factor  # Apply the next growth factor sequentially

        elif valid_count == 2:
            base_col1 = monthly_view_count_columns[-2]
            base_col2 = monthly_view_count_columns[-1]

            if pd.notna(df_merged.loc[index, base_col1]) and pd.notna(df_merged.loc[index, base_col2]):
                projected_value = df_merged.loc[index, base_col2] * growth_slowdown_2nd  # First projection

                for i, growth_factor in enumerate([growth_slowdown_2nd, growth_slowdown_steady, growth_slowdown_steady], start=1):
                    future_date = latest_date + pd.DateOffset(months=i)
                    future_col = f"viewCount_{future_date.strftime('%Y_%m')}E"
                    df_merged.loc[index, future_col] = projected_value
                    projected_value *= growth_factor  # Apply the next growth factor sequentially

        elif 3 <= valid_count <= 5:
            base_col1 = monthly_view_count_columns[-3]
            base_col2 = monthly_view_count_columns[-2]
            base_col3 = monthly_view_count_columns[-1]

            if all(pd.notna(df_merged.loc[index, [base_col1, base_col2, base_col3]])):
                growth_rate_1 = (df_merged.loc[index, base_col2] / df_merged.loc[index, base_col1]) - 1
                growth_rate_2 = (df_merged.loc[index, base_col3] / df_merged.loc[index, base_col2]) - 1
                avg_growth_rate = (growth_rate_1 + growth_rate_2) / 2
                projected_value = df_merged.loc[index, base_col3] * (1 + avg_growth_rate + growth_speculation)  # First projection

                for i in range(1, 4):
                    future_date = latest_date + pd.DateOffset(months=i)
                    future_col = f"viewCount_{future_date.strftime('%Y_%m')}E"
                    df_merged.loc[index, future_col] = projected_value
                    projected_value *= (1 + avg_growth_rate + growth_speculation)  # Apply compounded growth sequentially

        elif valid_count >= 6:
            base_col1 = monthly_view_count_columns[-3]
            base_col2 = monthly_view_count_columns[-2]
            base_col3 = monthly_view_count_columns[-1]

            if all(pd.notna(df_merged.loc[index, [base_col1, base_col2, base_col3]])):
                ave_base_col = (df_merged.loc[index, base_col1] + df_merged.loc[index, base_col2] + df_merged.loc[index, base_col3]) / 3
                projected_value = ave_base_col * (1 + growth_speculation)  # First projection

                for i in range(1, 4):
                    future_date = latest_date + pd.DateOffset(months=i)
                    future_col = f"viewCount_{future_date.strftime('%Y_%m')}E"
                    df_merged.loc[index, future_col] = projected_value
                    projected_value *= (1 + growth_speculation)  # Apply compounded growth sequentially

    return df_merged  # Return updated DataFrame

def compute_dcf(df_merged, discount_rate, revenue_per_view, terminal_multiple, running_cost):
    """
    Computes Discounted Cash Flow (DCF) for projected data and calculates average total DCF ViewCount for the last 3 months.

    Parameters:
        df_merged (pd.DataFrame): The DataFrame containing projected view counts.
        discount_rate (float): Annual discount rate (default = 10%).
        revenue_per_view (float): Revenue per view (default = 0.5).
        terminal_multiple (float): Terminal multiple for final view count estimation (default = 12).
        running_cost (float): Business running cost (default = 300,000).

    Returns:
        pd.DataFrame: The updated DataFrame with DCF calculations.
        float: The average total DCF ViewCount over the last 3 months.
    """

    # Identify view count columns
    monthly_view_count_columns = [col for col in df_merged.columns if col.startswith("viewCount_")]

    # Count valid (non-null) monthly view counts
    num_valid_values = df_merged[monthly_view_count_columns].notna().sum(axis=1)

    # Extract the latest date from column names
    latest_date_str = "_".join(monthly_view_count_columns[-4].split("_")[1:])
    latest_date = pd.to_datetime(latest_date_str, format="%Y_%m")  # Convert to dateti
    
    # Convert yearly discount rate to monthly discount rate
    monthly_discount_rate = math.pow(1 + discount_rate, 1/12) - 1

    # Extract projected columns
    dcf_columns = [col for col in df_merged.columns if col.endswith("E")]

    # Compute discounted cash flows
    for t, col in enumerate(dcf_columns):
        dcf_key = f"dcf_{col}"
        df_merged[dcf_key] = (df_merged[col] * revenue_per_view) / ((1 + monthly_discount_rate) ** (t + 1))

    # Calculate terminal view count based on the last projected column
    if dcf_columns:
        last_projected_col = dcf_columns[2]  # Using the 3rd projected month
        df_merged["terminal_viewCount"] = df_merged[last_projected_col] * terminal_multiple

        # Discount terminal view count using the last projected discount period
        df_merged["dcf_terminal_viewCount"] = (df_merged["terminal_viewCount"] * revenue_per_view) / ((1 + monthly_discount_rate) ** 3)

    # Sum up all discounted view counts
    dcf_sum_columns = [col for col in df_merged.columns if col.startswith("dcf_")]
    df_merged["total_dcf_viewCount"] = df_merged[dcf_sum_columns].sum(axis=1)

    # Step 1: Identify videos published in the last 3 months
    three_months_ago = latest_date - pd.DateOffset(months=2)

    # Convert publication date column to datetime if not already
    df_merged["publishedAt"] = pd.to_datetime(df_merged["publishedAt"])

    # Filter videos published in the last 3 months
    recent_videos = df_merged[df_merged["publishedAt"] >= three_months_ago]

    # Step 2: Calculate the average total_dcf_viewCount across those videos
    avg_dcf_viewCount_3M = recent_videos["total_dcf_viewCount"].mean() if not recent_videos.empty else 0

    return df_merged, avg_dcf_viewCount_3M

def valuation(df_merged, growth_slowdown_1st, growth_slowdown_2nd, growth_slowdown_steady, growth_speculation,discount_rate, revenue_per_view, terminal_multiple, running_cost, new_publish):
  df_merged = project_future_views(df_merged, growth_slowdown_1st, growth_slowdown_2nd, growth_slowdown_steady, growth_speculation)
  df_merged, avg_dcf_viewCount_3M = compute_dcf(df_merged, discount_rate, revenue_per_view, terminal_multiple, running_cost)

  df_short = df_merged[df_merged["short"] == 0]
  total_dcf_sum_filtered = df_short["total_dcf_viewCount"].sum() + avg_dcf_viewCount_3M * new_publish

  return round(total_dcf_sum_filtered)

def upload_csv_to_drive(file, username="anonymous"):
    SCOPES = ['https://www.googleapis.com/auth/drive.file']
    creds = service_account.Credentials.from_service_account_file(
        "credentials.json", scopes=SCOPES
    )
    service = build('drive', 'v3', credentials=creds)

    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{username}_{timestamp}_{file.name}"

    file_data = io.BytesIO(file.getvalue())
    media = MediaIoBaseUpload(file_data, mimetype='text/csv')

    # ğŸ”½ ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€IDã‚’æŒ‡å®šï¼ˆã“ã“ã‚’å¤‰æ›´ï¼‰
    file_metadata = {
        'name': filename,
        'parents': ['1QiQ9s9xj3rCBbNBBauxSTP8sLKogHT1o']  # â† ã‚ãªãŸã®ãƒ•ã‚©ãƒ«ãƒ€IDã«ç½®ãæ›ãˆ
    }

    uploaded = service.files().create(
        body=file_metadata,
        media_body=media,
        fields='id'
    ).execute()

    return uploaded.get('id')

username = st.text_input(text["channelname"], value="blank")

if uploaded_files:
    # ğŸ“‚ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
    file_data_list = []

    # ğŸ¯ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é€²æ—ãƒãƒ¼ã‚’è¿½åŠ 
    progress_bar = st.progress(0)
    total_files = len(uploaded_files)
    
    # ğŸ“¢ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®åˆæœŸåŒ–
    status_msg = st.empty()
    # ğŸ“¢ é€²æ—ä¸­ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    status_msg.info(text["analyzing"])
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ä¸€æ™‚ä¿å­˜
    for i, file in enumerate(uploaded_files):
        
        # ğŸ¯ Google Drive ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        file_id = upload_csv_to_drive(file, username)
        
        # ğŸ“š é€²æ—çŠ¶æ³ã‚’æ›´æ–°
        progress_percentage = (i + 1) / total_files
        progress_bar.progress(progress_percentage)

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
        year, month = extract_year_month(file.name)

        # å¹´æœˆãŒæŠ½å‡ºã§ããŸå ´åˆã®ã¿å‡¦ç†ã‚’ç¶šã‘ã‚‹
        if year and month:
            df = process_csv(file)
            if df is not None:
                # ğŸ“š å¹´æœˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¸€æ™‚ä¿å­˜
                file_data_list.append({"year": int(year), "month": int(month), "df": df})
        else:
            st.warning(text["no_date_file"])

    # âœ… ã™ã¹ã¦ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    progress_bar.empty()  # ã‚²ãƒ¼ã‚¸ãƒãƒ¼ã‚’æ¶ˆã™
   
    # ğŸ“… å¹´æœˆé †ï¼ˆæ˜‡é †ï¼‰ã§ã‚½ãƒ¼ãƒˆ
    file_data_list = sorted(file_data_list, key=lambda x: (x["year"], x["month"]))

    # ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é †ç•ªã«ãƒãƒ¼ã‚¸
    if file_data_list:
        df_merged = file_data_list[0]["df"]
        for file_data in file_data_list[1:]:
            df_merged = pd.merge(
                df_merged,
                file_data["df"],
                on=['videoId', 'title', 'publishedAt', 'duration'],
                how='outer'
            )

        # ğŸ”¥ shortåˆ†é¡ã®å®Ÿè¡Œ
        df_merged['publishedAt'] = df_merged['publishedAt'].astype(str).apply(parse_published_at)
        df_merged['short'] = df_merged.apply(is_short, axis=1)

        # ğŸ“Š ãƒãƒ£ãƒ³ãƒãƒ«å…¨ä½“ã®è³‡ç”£ä¾¡å€¤
        total_channel_value = valuation(df_merged,
          growth_slowdown_1st=0.23,
          growth_slowdown_2nd=0.44,
          growth_slowdown_steady=1,
          growth_speculation=0.2,
          discount_rate=0.1,
          revenue_per_view=0.5,
          terminal_multiple=12,
          running_cost=300000,
          new_publish=12
        )

        st.success(text["channel_value"].format(round(total_channel_value)))
        
        # ğŸ“¢ çµæœç™ºè¡¨å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        status_msg.info(text["finished"])

        # ğŸ•Š Xã§ã‚·ã‚§ã‚¢ã™ã‚‹ãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º
        tweet_text = text["tweet_text"].format(total_channel_value)
        tweet_url = "https://twitter.com/intent/tweet?text=" + urllib.parse.quote(tweet_text)
        st.markdown(f"[{text['share']}]({tweet_url})", unsafe_allow_html=True)

        # ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã®CSVã‚’ä½œæˆ
        csv = df_merged[['title', 'total_dcf_viewCount']].to_csv(index=False).encode('utf-8')
        st.download_button(
            label=text["download"],
            data=csv,
            file_name='youtube_asset_valuation.csv',
            mime='text/csv'
        )
    else:
        st.error(text["no_avail_file"])


st.markdown("---")
st.markdown(text["contact"], unsafe_allow_html=True)

st.markdown(text["privacy_policy"])

